{"title":"Algorithmic Decision Making & Census Data","markdown":{"yaml":{"title":"Algorithmic Decision Making & Census Data","subtitle":"Week 2: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"September 15, 2025","format":{"revealjs":{"theme":"simple","slide-number":true,"scrollable":true,"chalkboard":true,"code-line-numbers":true,"incremental":false}}},"headingText":"Today's Agenda","containsRefs":false,"markdown":"\n\n\n## What We'll Cover\n\n**Part 1: Algorithmic Decision Making** \n\n- What are algorithms in public policy?\n- When algorithmic decision making goes wrong\n- Current policy responses\n\n**Part 2: Active Learning**  \n\n- Small group scenarios: designing ethical algorithms\n- Discussion and reflection\n\n**Part 3: Census Data Foundations** \n\n- Understanding census data for policy analysis\n- Geography and data availability\n\n**Part 4: Hands-On Census Data with R** \n\n- Live demonstration of key functions\n- Practice exercises\n\n---\n\n# Part 1: Algorithmic Decision Making\n\n## Opening Question\n\n**Discuss with your table (1 minutes):**\n\nWhat is an algorithm?\n\n*Think beyond just computer code - how do you make decisions in daily life?*\n\n## What Is An Algorithm?\n\n**Definition:** A set of rules or instructions for solving a problem or completing a task\n\n**Examples:**\n\n- Recipe for cooking\n- Directions to get somewhere  \n- Decision tree for hiring\n\n- **Computer program that processes data to make predictions**\n\n## Algorithmic Decision Making in Government\n\n**Systems used to assist or replace human decision-makers**\n\nBased on predictions from models that process historical data containing:\n\n- **Inputs** (\"features\", \"predictors\", \"independent variables\", \"x\")\n- **Outputs** (\"labels\", \"outcome\", \"dependent variable\", \"y\")\n\n## Real-World Examples\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n**Criminal Justice**\nRecidivism risk scores for bail and sentencing decisions\n:::\n\n::: {.column width=\"30%\"}\n\n**Housing & Finance**  \nMortgage lending and tenant screening algorithms\n:::\n\n::: {.column width=\"30%\"}\n\n**Healthcare**\nPatient care prioritization and resource allocation\n:::\n\n::::\n\n## Clarifying Key Terms\n\n**Data Science** → Computer science/engineering focus on algorithms and methods\n\n**Data Analytics** → Application of data science methods to other disciplines  \n\n**Machine Learning** → Algorithms for classification & prediction that learn from data\n\n**AI** → Algorithms that adjust and improve across iterations (neural networks, etc.)\n\n## Public Sector Context\n\n**Long history of government data collection:**\n\n- Civic registration systems  \n- Census data\n- Administrative records\n- Operations research (post-WWII)\n\n**What's new?**\n\n- More data (official and \"accidental\")\n- Focus on **prediction** rather than explanation\n- **Harder to interpret** and explain\n\n## Why Government Uses Algorithms\n\n**Governments have limited budgets and need to serve everyone**\n\nAlgorithmic decision making is especially appealing because it promises:\n\n- **Efficiency** - process more cases faster\n- **Consistency** - same rules applied to everyone  \n- **Objectivity** - removes human bias\n- **Cost savings** - fewer staff needed\n\n**But does it deliver on these promises?**\n\n---\n\n# When Algorithms Go Wrong\n\n## Remember: Data Analytics Is Subjective\n\n**Every step involves human choices:**\n\n- Data cleaning decisions\n- Data coding or classification  \n- Data collection - use of imperfect proxies\n- How you interpret results\n- What variables you put in the model\n\n**These choices embed human values and biases**\n\n## {.smaller}\n**Healthcare Algorithm Bias**\n\n**The Problem:**\n\nAlgorithm used to identify high-risk patients for additional care **systematically discriminated against Black patients**\n\n**What Went Wrong:**\n\n- Algorithm used **healthcare costs as a proxy for need**\n- Black patients typically incur lower costs due to **systemic inequities in access**  \n- Result: Black patients under-prioritized despite **equivalent levels of illness**\n\n**Scale:** Used by hospitals and insurers for **over 200 million people** annually\n\n*Source: Obermeyer et al. (2019), Science*\n\n## Criminal Justice Algorithm Bias  \n\n**COMPAS Recidivism Prediction:**\n\n**The Problem:**\n\n- Algorithm **2x as likely** to falsely flag Black defendants as high risk\n- White defendants often rated low risk **even when they do reoffend**  \n\n**Why This Happens:**\n\n- Historical arrest data reflects **biased policing patterns**\n- Socioeconomic proxies correlate with race\n\n- **\"Objective\" data contains subjective human decisions**\n\n*Source: ProPublica investigation*\n\n## Dutch Welfare Fraud Detection\n\n**The Problem:**\n\n- **\"Black box\" system** operated in secrecy\n- Impossible for individuals to understand or challenge decisions\n- **Disproportionately targeted** vulnerable populations\n\n**Court Ruling:**\n\n- Breached privacy rights under European Convention on Human Rights\n- Highlighted **unfair profiling and discrimination**\n- System eventually **shut down**\n\n\n# Part 2: Active Learning Exercise\n\n## Small Group Challenge (10 Minutes! We keep a tight ship around here.)\n\nAt your table, pick **one** scenario and answer **three** prompts.\n\n**Prompts (plain English, no tech):**\n\n1. **Proxy:** What would you *use* to stand in for what you *want*?\n2. **Blind spot:** What data gap or historical bias could skew results?\n3. **Harm + Guardrail:** Who could be harmed, and one simple safeguard?\n\n\n## Pick one scenario\n\n1) **Emergency response prioritization** (natural disasters)  \n2) **School enrollment assignment**  \n3) **Automated traffic enforcement** (red-light cameras)  \n4) **Housing assistance allocation**  \n5) **Predictive policing**\n\n## Example (so you see the level)\n\n**Scenario:** Emergency response  \n\n- **Proxy:** 911 call volume → stand-in for “need”  \n- **Blind spot:** Under-calling where trust/connectivity is low  \n- **Harm + Guardrail:** Wealthier areas over-prioritized → add a **vulnerability boost** (age/disability) and a **minimum-service floor** per zone\n\n## Discuss at your table (8 minutes)\n\n**Answer these out loud and on one device or notepad:**\n\n- **Proxy →** “We’d use ____ as a stand-in for ____.”\n- **Blind spot →** “This could miss/undercount ____ because ____.”\n- **Harm + Guardrail →** “Group ____ could be hurt by ____. We’d add ____ (one safeguard).”\n\n**Choose ONE guardrail type:**\n\n- Prioritize vulnerable groups  \n- Cap disparities across areas (simple rule)  \n- Human review + appeals for edge cases  \n- Replace a bad proxy (collect the right thing)  \n- Publish criteria & run a periodic bias check\n\n## Lightning shares (2–3 tables)\n\nIn **≤20 seconds**, say:\n\n- **Scenario**, **one proxy**, **one harm**, **one guardrail**\n\nClass quick poll: *Would that guardrail help?* \n\n- 👍 Green light \n- 👎 Red light\n\n\n---\n\n# Part 3: Census Data Foundations\n\n## Why Census Data Matters\n\n**Census data is the foundation for:**\n\n- Understanding community demographics\n- Allocating government resources  \n- Tracking neighborhood change\n\n- **Designing fair algorithms** (like those we just discussed)\n\n**Connection:** The same demographic data used in census goes into many of the algorithms we analyzed\n\n## Census vs. American Community Survey\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Decennial Census (2020)**\n\n- **Everyone** counted every 10 years\n- **9 basic questions:** age, race, sex, housing\n- **Constitutional requirement**\n- Determines political representation\n:::\n\n::: {.column width=\"50%\"}  \n**American Community Survey (ACS)**\n\n- **3% of households** surveyed annually\n- **Detailed questions:** income, education, employment, housing costs\n- **Replaced** the old \"long form\" in 2005\n- **A big source of data we'll use** this semester\n:::\n\n::::\n\n## ACS Estimates: What You Need to Know\n\n**1-Year Estimates** (areas > 65,000 people)\n\n- Most current data, smallest sample\n\n**5-Year Estimates** (all areas including census tracts)  \n\n- Most reliable data, largest sample\n- **What you'll use most often**\n\n**Key Point:** All ACS data comes with **margins of error** - we'll learn to work with uncertainty\n\n## Census Geography Hierarchy\n\n```\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\n```\n\n**Most policy analysis happens at:**\n\n- **County level** - state and regional planning\n- **Census tract level** - neighborhood analysis \n- **Block group level** - very local analysis (tempting, but big MOEs)\n\n## 2020 Census Innovation: Differential Privacy\n\n**The Challenge:** Modern computing can \"re-identify\" individuals from census data\n\n**The Solution:** Add mathematical \"noise\" to protect privacy while preserving overall patterns\n\n**The Controversy:** Some places now show populations living \"underwater\" or other impossible results\n\n**Why This Matters:** Even \"objective\" data involves **subjective choices** about privacy vs. accuracy. Also, the errors.\n\n## Accessing Census Data in R\n\n**Traditional approach:** Download CSV files from Census website\n\n**Modern approach:** Use R packages to access data directly\n\n**Benefits of programmatic access:**\n\n- Always get latest data\n- Reproducible workflows  \n- Automatic geographic boundaries\n- Built-in error handling\n\n**We'll use the `tidycensus` package starting in Lab 1**\n\n## Understanding ACS Data Structure\n\n**Data organized in tables:**\n\n- **B19013** - Median Household Income\n- **B25003** - Housing Tenure (Own/Rent)  \n- **B15003** - Educational Attainment\n- **B08301** - Commuting to Work\n\n**Each table has multiple variables:**\n\n- B19013_001E = Median household income (estimate)\n- B19013_001M = Median household income (margin of error)\n\n**You'll learn to find the right variables for your research questions**\n\n## Working with Margins of Error\n\n**Every ACS estimate comes with uncertainty**\n\n**Rule of thumb:**\n\n- Large MOE relative to estimate = **less reliable**\n- Small MOE relative to estimate = **more reliable**\n\n**In your analysis:**\n\n- Always report MOE alongside estimates\n- Be cautious comparing estimates with overlapping error margins\n- Consider using 5-year estimates for greater reliability\n\n## Two Types of Census Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Summary Tables** (what we'll use mostly)\n\n- Pre-calculated statistics by geography\n- Median income, percent college-educated, etc.\n- **Good for:** Mapping, comparing places\n:::\n\n::: {.column width=\"50%\"}  \n**PUMS - Individual Records** \n\n- Anonymous individual/household responses\n- **Good for:** Custom analysis, regression models\n- More complex but more flexible\n:::\n\n::::\n\n## When New Data Comes Out\n\n**ACS 1-year estimates:** Released in September (previous year's data)\n\n**ACS 5-year estimates:** Released in December  \n\n**Decennial Census:** Released on rolling schedule over 2-3 years\n\n**For Lab 1:** We'll use 2018-2022 ACS 5-year estimates (latest available)\n\n## Data Sources You'll Use\n\n**TIGER/Line Files** \n\n- Geographic boundaries (shapefiles)\n- Census tracts, counties, states\n- Now released as shapefiles (easier to use!)\n\n**Historical Data Sources:**\n\n- NHGIS (nhgis.org) - Historical census data\n- Neighborhood Change Database \n- Longitudinal Tract Database - Track changes over time\n\n---\n\n# Part 4: Hands-On Census Data with R\n\n## Live Demo Setup\n\nLet's see tidycensus in action with some basic examples:\n\n```{r demo-setup}\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\n# Set API key (you'll get yours for Lab 1)\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n```\n\n**Follow along:** We'll work through examples together, then you'll practice in Lab 1\n\n## Basic get_acs() Function\n\n**Most important function you'll use:**\n\n```{r demo-get-acs}\n# Get state-level population data\nstate_pop <- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",  # Total population\n  year = 2022,\n  survey = \"acs5\"\n)\n\nglimpse(state_pop)\n```\n\n**Key parameters:** geography, variables, year, survey\n\n## Understanding the Output\n\n**Every ACS result includes:**\n\n- `GEOID` - Geographic identifier\n- `NAME` - Human-readable location name  \n- `variable` - Census variable code\n- `estimate` - The actual value\n- `moe` - Margin of error\n\n**This is the foundation for all your analysis**\n\n## Working with Multiple Variables\n\n```{r demo-multiple-vars}\n# Get income and population for Pennsylvania counties\npa_data <- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"  # Makes analysis easier\n)\n\nhead(pa_data)\n```\n\n**Note:** `output = \"wide\"` gives you one row per place, multiple columns for variables\n\n## Data Cleaning Essentials\n\n**Clean up messy geographic names:**\n\n```{r demo-cleaning}\npa_clean <- pa_data %>%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    # Remove \"County\" word\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Compare before and after\nselect(pa_clean, NAME, county_name)\n```\n\n**Functions you'll use:** `str_remove()`, `str_extract()`, `str_replace()`\n\n## Calculating Data Reliability\n\n**This is crucial for policy work:**\n\n```{r demo-moe}\npa_reliability <- pa_clean %>%\n  mutate(\n    # Calculate MOE as percentage of estimate\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 2),\n    \n    # Create reliability categories\n    reliability = case_when(\n      moe_percentage < 5 ~ \"High Confidence\",\n      moe_percentage >= 5 & moe_percentage <= 10 ~ \"Moderate\",\n      moe_percentage > 10 ~ \"Low Confidence\"\n    )\n  )\n\ncount(pa_reliability, reliability)\n```\n\n**Key functions:** `case_when()` for categories, MOE calculations\n\n## Finding Patterns with dplyr\n\n- Find counties with highest uncertainty\n- Summarize by reliability category\n\n```{r demo-analysis}\n# Find counties with highest uncertainty\nhigh_uncertainty <- pa_reliability %>%\n  filter(moe_percentage > 8) %>%\n  arrange(desc(moe_percentage)) %>%\n  select(county_name, median_incomeE, moe_percentage)\n\n# Summary by reliability category  \nreliability_summary <- pa_reliability %>%\n  group_by(reliability) %>%\n  summarize(\n    counties = n(),\n    avg_income = round(mean(median_incomeE, na.rm = TRUE), 0)\n  )\n```\n\n## Professional Tables\n\n**Making results presentation-ready:**\n\n```{r demo-tables}\n# Create formatted table\nkable(high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"MOE %\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n```\n\n**Key points:**\n\n- Use `kable()` for professional formatting\n- Add descriptive column names and captions  \n- Format numbers appropriately\n\n## Quick Practice\n\n**Try this with a neighbor (5 minutes):**\n\nUsing the `pa_reliability` data we just created:\n\n1. **Filter** for counties with \"High Confidence\" data\n2. **Arrange** by median income (highest first)  \n3. **Select** county name and median income\n4. **Slice** the top 3 counties\n\n**We'll share answers before moving on**\n\n## Policy Connection\n\n**Why this matters:**\n\n- **Algorithmic fairness:** Unreliable data can bias automated decisions\n- **Resource allocation:** Know which areas need extra attention\n- **Equity analysis:** Some communities may be systematically under-counted\n- **Professional credibility:** Always assess your data quality\n\n**This connects directly to our algorithmic bias discussion**\n\n---\n\n# Connecting the Dots\n\n## From Algorithms to Analysis\n\n**Today's key connections:**\n\n**Algorithmic Decision Making** → Understanding why your analysis matters for real policy decisions\n\n**Data Subjectivity** → Why we emphasize **transparent, reproducible methods** in this class\n\n**Census Data** → The foundation for most urban planning and policy analysis\n\n**R Skills** → The tools to do this work professionally and ethically\n\n## Questions for Reflection\n\n**As you work with data this semester, ask:**\n\n1. **What assumptions am I making** in my data choices?\n2. **Who might be excluded** from my analysis?\n3. **How could my findings be misused** if taken out of context?\n4. **What would I want policymakers to understand** about my methods?\n\nThese questions will make you a **more thoughtful analyst** and **better future policymaker**\n\n---\n\n# Next Steps\n\n## Before Next Class\n\n1. **Complete Lab 0** if you haven't finished\n2. **Post your weekly notes** - reflect on today's discussion\n3. **Start Lab 1** - census data exploration (begins today!)\n\n\n## Questions?\n\n","srcMarkdownNoYaml":"\n\n# Today's Agenda\n\n## What We'll Cover\n\n**Part 1: Algorithmic Decision Making** \n\n- What are algorithms in public policy?\n- When algorithmic decision making goes wrong\n- Current policy responses\n\n**Part 2: Active Learning**  \n\n- Small group scenarios: designing ethical algorithms\n- Discussion and reflection\n\n**Part 3: Census Data Foundations** \n\n- Understanding census data for policy analysis\n- Geography and data availability\n\n**Part 4: Hands-On Census Data with R** \n\n- Live demonstration of key functions\n- Practice exercises\n\n---\n\n# Part 1: Algorithmic Decision Making\n\n## Opening Question\n\n**Discuss with your table (1 minutes):**\n\nWhat is an algorithm?\n\n*Think beyond just computer code - how do you make decisions in daily life?*\n\n## What Is An Algorithm?\n\n**Definition:** A set of rules or instructions for solving a problem or completing a task\n\n**Examples:**\n\n- Recipe for cooking\n- Directions to get somewhere  \n- Decision tree for hiring\n\n- **Computer program that processes data to make predictions**\n\n## Algorithmic Decision Making in Government\n\n**Systems used to assist or replace human decision-makers**\n\nBased on predictions from models that process historical data containing:\n\n- **Inputs** (\"features\", \"predictors\", \"independent variables\", \"x\")\n- **Outputs** (\"labels\", \"outcome\", \"dependent variable\", \"y\")\n\n## Real-World Examples\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n**Criminal Justice**\nRecidivism risk scores for bail and sentencing decisions\n:::\n\n::: {.column width=\"30%\"}\n\n**Housing & Finance**  \nMortgage lending and tenant screening algorithms\n:::\n\n::: {.column width=\"30%\"}\n\n**Healthcare**\nPatient care prioritization and resource allocation\n:::\n\n::::\n\n## Clarifying Key Terms\n\n**Data Science** → Computer science/engineering focus on algorithms and methods\n\n**Data Analytics** → Application of data science methods to other disciplines  \n\n**Machine Learning** → Algorithms for classification & prediction that learn from data\n\n**AI** → Algorithms that adjust and improve across iterations (neural networks, etc.)\n\n## Public Sector Context\n\n**Long history of government data collection:**\n\n- Civic registration systems  \n- Census data\n- Administrative records\n- Operations research (post-WWII)\n\n**What's new?**\n\n- More data (official and \"accidental\")\n- Focus on **prediction** rather than explanation\n- **Harder to interpret** and explain\n\n## Why Government Uses Algorithms\n\n**Governments have limited budgets and need to serve everyone**\n\nAlgorithmic decision making is especially appealing because it promises:\n\n- **Efficiency** - process more cases faster\n- **Consistency** - same rules applied to everyone  \n- **Objectivity** - removes human bias\n- **Cost savings** - fewer staff needed\n\n**But does it deliver on these promises?**\n\n---\n\n# When Algorithms Go Wrong\n\n## Remember: Data Analytics Is Subjective\n\n**Every step involves human choices:**\n\n- Data cleaning decisions\n- Data coding or classification  \n- Data collection - use of imperfect proxies\n- How you interpret results\n- What variables you put in the model\n\n**These choices embed human values and biases**\n\n## {.smaller}\n**Healthcare Algorithm Bias**\n\n**The Problem:**\n\nAlgorithm used to identify high-risk patients for additional care **systematically discriminated against Black patients**\n\n**What Went Wrong:**\n\n- Algorithm used **healthcare costs as a proxy for need**\n- Black patients typically incur lower costs due to **systemic inequities in access**  \n- Result: Black patients under-prioritized despite **equivalent levels of illness**\n\n**Scale:** Used by hospitals and insurers for **over 200 million people** annually\n\n*Source: Obermeyer et al. (2019), Science*\n\n## Criminal Justice Algorithm Bias  \n\n**COMPAS Recidivism Prediction:**\n\n**The Problem:**\n\n- Algorithm **2x as likely** to falsely flag Black defendants as high risk\n- White defendants often rated low risk **even when they do reoffend**  \n\n**Why This Happens:**\n\n- Historical arrest data reflects **biased policing patterns**\n- Socioeconomic proxies correlate with race\n\n- **\"Objective\" data contains subjective human decisions**\n\n*Source: ProPublica investigation*\n\n## Dutch Welfare Fraud Detection\n\n**The Problem:**\n\n- **\"Black box\" system** operated in secrecy\n- Impossible for individuals to understand or challenge decisions\n- **Disproportionately targeted** vulnerable populations\n\n**Court Ruling:**\n\n- Breached privacy rights under European Convention on Human Rights\n- Highlighted **unfair profiling and discrimination**\n- System eventually **shut down**\n\n\n# Part 2: Active Learning Exercise\n\n## Small Group Challenge (10 Minutes! We keep a tight ship around here.)\n\nAt your table, pick **one** scenario and answer **three** prompts.\n\n**Prompts (plain English, no tech):**\n\n1. **Proxy:** What would you *use* to stand in for what you *want*?\n2. **Blind spot:** What data gap or historical bias could skew results?\n3. **Harm + Guardrail:** Who could be harmed, and one simple safeguard?\n\n\n## Pick one scenario\n\n1) **Emergency response prioritization** (natural disasters)  \n2) **School enrollment assignment**  \n3) **Automated traffic enforcement** (red-light cameras)  \n4) **Housing assistance allocation**  \n5) **Predictive policing**\n\n## Example (so you see the level)\n\n**Scenario:** Emergency response  \n\n- **Proxy:** 911 call volume → stand-in for “need”  \n- **Blind spot:** Under-calling where trust/connectivity is low  \n- **Harm + Guardrail:** Wealthier areas over-prioritized → add a **vulnerability boost** (age/disability) and a **minimum-service floor** per zone\n\n## Discuss at your table (8 minutes)\n\n**Answer these out loud and on one device or notepad:**\n\n- **Proxy →** “We’d use ____ as a stand-in for ____.”\n- **Blind spot →** “This could miss/undercount ____ because ____.”\n- **Harm + Guardrail →** “Group ____ could be hurt by ____. We’d add ____ (one safeguard).”\n\n**Choose ONE guardrail type:**\n\n- Prioritize vulnerable groups  \n- Cap disparities across areas (simple rule)  \n- Human review + appeals for edge cases  \n- Replace a bad proxy (collect the right thing)  \n- Publish criteria & run a periodic bias check\n\n## Lightning shares (2–3 tables)\n\nIn **≤20 seconds**, say:\n\n- **Scenario**, **one proxy**, **one harm**, **one guardrail**\n\nClass quick poll: *Would that guardrail help?* \n\n- 👍 Green light \n- 👎 Red light\n\n\n---\n\n# Part 3: Census Data Foundations\n\n## Why Census Data Matters\n\n**Census data is the foundation for:**\n\n- Understanding community demographics\n- Allocating government resources  \n- Tracking neighborhood change\n\n- **Designing fair algorithms** (like those we just discussed)\n\n**Connection:** The same demographic data used in census goes into many of the algorithms we analyzed\n\n## Census vs. American Community Survey\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Decennial Census (2020)**\n\n- **Everyone** counted every 10 years\n- **9 basic questions:** age, race, sex, housing\n- **Constitutional requirement**\n- Determines political representation\n:::\n\n::: {.column width=\"50%\"}  \n**American Community Survey (ACS)**\n\n- **3% of households** surveyed annually\n- **Detailed questions:** income, education, employment, housing costs\n- **Replaced** the old \"long form\" in 2005\n- **A big source of data we'll use** this semester\n:::\n\n::::\n\n## ACS Estimates: What You Need to Know\n\n**1-Year Estimates** (areas > 65,000 people)\n\n- Most current data, smallest sample\n\n**5-Year Estimates** (all areas including census tracts)  \n\n- Most reliable data, largest sample\n- **What you'll use most often**\n\n**Key Point:** All ACS data comes with **margins of error** - we'll learn to work with uncertainty\n\n## Census Geography Hierarchy\n\n```\nNation\n├── Regions  \n├── States\n│   ├── Counties\n│   │   ├── Census Tracts (1,500-8,000 people)\n│   │   │   ├── Block Groups (600-3,000 people)  \n│   │   │   │   └── Blocks (≈85 people, Decennial only)\n```\n\n**Most policy analysis happens at:**\n\n- **County level** - state and regional planning\n- **Census tract level** - neighborhood analysis \n- **Block group level** - very local analysis (tempting, but big MOEs)\n\n## 2020 Census Innovation: Differential Privacy\n\n**The Challenge:** Modern computing can \"re-identify\" individuals from census data\n\n**The Solution:** Add mathematical \"noise\" to protect privacy while preserving overall patterns\n\n**The Controversy:** Some places now show populations living \"underwater\" or other impossible results\n\n**Why This Matters:** Even \"objective\" data involves **subjective choices** about privacy vs. accuracy. Also, the errors.\n\n## Accessing Census Data in R\n\n**Traditional approach:** Download CSV files from Census website\n\n**Modern approach:** Use R packages to access data directly\n\n**Benefits of programmatic access:**\n\n- Always get latest data\n- Reproducible workflows  \n- Automatic geographic boundaries\n- Built-in error handling\n\n**We'll use the `tidycensus` package starting in Lab 1**\n\n## Understanding ACS Data Structure\n\n**Data organized in tables:**\n\n- **B19013** - Median Household Income\n- **B25003** - Housing Tenure (Own/Rent)  \n- **B15003** - Educational Attainment\n- **B08301** - Commuting to Work\n\n**Each table has multiple variables:**\n\n- B19013_001E = Median household income (estimate)\n- B19013_001M = Median household income (margin of error)\n\n**You'll learn to find the right variables for your research questions**\n\n## Working with Margins of Error\n\n**Every ACS estimate comes with uncertainty**\n\n**Rule of thumb:**\n\n- Large MOE relative to estimate = **less reliable**\n- Small MOE relative to estimate = **more reliable**\n\n**In your analysis:**\n\n- Always report MOE alongside estimates\n- Be cautious comparing estimates with overlapping error margins\n- Consider using 5-year estimates for greater reliability\n\n## Two Types of Census Data\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n**Summary Tables** (what we'll use mostly)\n\n- Pre-calculated statistics by geography\n- Median income, percent college-educated, etc.\n- **Good for:** Mapping, comparing places\n:::\n\n::: {.column width=\"50%\"}  \n**PUMS - Individual Records** \n\n- Anonymous individual/household responses\n- **Good for:** Custom analysis, regression models\n- More complex but more flexible\n:::\n\n::::\n\n## When New Data Comes Out\n\n**ACS 1-year estimates:** Released in September (previous year's data)\n\n**ACS 5-year estimates:** Released in December  \n\n**Decennial Census:** Released on rolling schedule over 2-3 years\n\n**For Lab 1:** We'll use 2018-2022 ACS 5-year estimates (latest available)\n\n## Data Sources You'll Use\n\n**TIGER/Line Files** \n\n- Geographic boundaries (shapefiles)\n- Census tracts, counties, states\n- Now released as shapefiles (easier to use!)\n\n**Historical Data Sources:**\n\n- NHGIS (nhgis.org) - Historical census data\n- Neighborhood Change Database \n- Longitudinal Tract Database - Track changes over time\n\n---\n\n# Part 4: Hands-On Census Data with R\n\n## Live Demo Setup\n\nLet's see tidycensus in action with some basic examples:\n\n```{r demo-setup}\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\n# Set API key (you'll get yours for Lab 1)\ncensus_api_key(\"42bf8a20a3df1def380f330cf7edad0dd5842ce6\")\n```\n\n**Follow along:** We'll work through examples together, then you'll practice in Lab 1\n\n## Basic get_acs() Function\n\n**Most important function you'll use:**\n\n```{r demo-get-acs}\n# Get state-level population data\nstate_pop <- get_acs(\n  geography = \"state\",\n  variables = \"B01003_001\",  # Total population\n  year = 2022,\n  survey = \"acs5\"\n)\n\nglimpse(state_pop)\n```\n\n**Key parameters:** geography, variables, year, survey\n\n## Understanding the Output\n\n**Every ACS result includes:**\n\n- `GEOID` - Geographic identifier\n- `NAME` - Human-readable location name  \n- `variable` - Census variable code\n- `estimate` - The actual value\n- `moe` - Margin of error\n\n**This is the foundation for all your analysis**\n\n## Working with Multiple Variables\n\n```{r demo-multiple-vars}\n# Get income and population for Pennsylvania counties\npa_data <- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"  # Makes analysis easier\n)\n\nhead(pa_data)\n```\n\n**Note:** `output = \"wide\"` gives you one row per place, multiple columns for variables\n\n## Data Cleaning Essentials\n\n**Clean up messy geographic names:**\n\n```{r demo-cleaning}\npa_clean <- pa_data %>%\n  mutate(\n    # Remove state name from county names\n    county_name = str_remove(NAME, \", Pennsylvania\"),\n    # Remove \"County\" word\n    county_name = str_remove(county_name, \" County\")\n  )\n\n# Compare before and after\nselect(pa_clean, NAME, county_name)\n```\n\n**Functions you'll use:** `str_remove()`, `str_extract()`, `str_replace()`\n\n## Calculating Data Reliability\n\n**This is crucial for policy work:**\n\n```{r demo-moe}\npa_reliability <- pa_clean %>%\n  mutate(\n    # Calculate MOE as percentage of estimate\n    moe_percentage = round((median_incomeM / median_incomeE) * 100, 2),\n    \n    # Create reliability categories\n    reliability = case_when(\n      moe_percentage < 5 ~ \"High Confidence\",\n      moe_percentage >= 5 & moe_percentage <= 10 ~ \"Moderate\",\n      moe_percentage > 10 ~ \"Low Confidence\"\n    )\n  )\n\ncount(pa_reliability, reliability)\n```\n\n**Key functions:** `case_when()` for categories, MOE calculations\n\n## Finding Patterns with dplyr\n\n- Find counties with highest uncertainty\n- Summarize by reliability category\n\n```{r demo-analysis}\n# Find counties with highest uncertainty\nhigh_uncertainty <- pa_reliability %>%\n  filter(moe_percentage > 8) %>%\n  arrange(desc(moe_percentage)) %>%\n  select(county_name, median_incomeE, moe_percentage)\n\n# Summary by reliability category  \nreliability_summary <- pa_reliability %>%\n  group_by(reliability) %>%\n  summarize(\n    counties = n(),\n    avg_income = round(mean(median_incomeE, na.rm = TRUE), 0)\n  )\n```\n\n## Professional Tables\n\n**Making results presentation-ready:**\n\n```{r demo-tables}\n# Create formatted table\nkable(high_uncertainty,\n      col.names = c(\"County\", \"Median Income\", \"MOE %\"),\n      caption = \"Counties with Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n```\n\n**Key points:**\n\n- Use `kable()` for professional formatting\n- Add descriptive column names and captions  \n- Format numbers appropriately\n\n## Quick Practice\n\n**Try this with a neighbor (5 minutes):**\n\nUsing the `pa_reliability` data we just created:\n\n1. **Filter** for counties with \"High Confidence\" data\n2. **Arrange** by median income (highest first)  \n3. **Select** county name and median income\n4. **Slice** the top 3 counties\n\n**We'll share answers before moving on**\n\n## Policy Connection\n\n**Why this matters:**\n\n- **Algorithmic fairness:** Unreliable data can bias automated decisions\n- **Resource allocation:** Know which areas need extra attention\n- **Equity analysis:** Some communities may be systematically under-counted\n- **Professional credibility:** Always assess your data quality\n\n**This connects directly to our algorithmic bias discussion**\n\n---\n\n# Connecting the Dots\n\n## From Algorithms to Analysis\n\n**Today's key connections:**\n\n**Algorithmic Decision Making** → Understanding why your analysis matters for real policy decisions\n\n**Data Subjectivity** → Why we emphasize **transparent, reproducible methods** in this class\n\n**Census Data** → The foundation for most urban planning and policy analysis\n\n**R Skills** → The tools to do this work professionally and ethically\n\n## Questions for Reflection\n\n**As you work with data this semester, ask:**\n\n1. **What assumptions am I making** in my data choices?\n2. **Who might be excluded** from my analysis?\n3. **How could my findings be misused** if taken out of context?\n4. **What would I want policymakers to understand** about my methods?\n\nThese questions will make you a **more thoughtful analyst** and **better future policymaker**\n\n---\n\n# Next Steps\n\n## Before Next Class\n\n1. **Complete Lab 0** if you haven't finished\n2. **Post your weekly notes** - reflect on today's discussion\n3. **Start Lab 1** - census data exploration (begins today!)\n\n\n## Questions?\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":false,"output-file":"week2_slides.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.8.24","auto-stretch":true,"title":"Algorithmic Decision Making & Census Data","subtitle":"Week 2: MUSA 5080","author":"Dr. Elizabeth Delmelle","date":"September 15, 2025","theme":"simple","slideNumber":true,"scrollable":true,"chalkboard":true}}},"projectFormats":["html"]}